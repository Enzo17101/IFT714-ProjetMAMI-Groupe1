{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import default_data_collator\n",
    "from transformers import BertConfig, BertForSequenceClassification\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ],
   "id": "2400ebf221fe671f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hyperparamètres",
   "id": "2b350f28aac359f0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Tokeniser\n",
    "max_length = 256 # Longueur maximale des séquences tokenisées\n",
    "\n",
    "# Capacité du modèle\n",
    "batch_size = 16 # Taille des batchs\n",
    "num_epochs = 3 # Nombre d'époques\n",
    "\n",
    "# Dropout\n",
    "dropout_global = 0.3 # Dropout global\n",
    "dropout_attention = 0.4 # Dropout dans les couches d'attention\n",
    "\n",
    "# Autres paramètres du modèle\n",
    "#learning_rate = 2e-5 # Taux d'apprentissage\n",
    "weight_decay = 0.1 # Paramètre de régularisation L2\n",
    "warmup_steps = 0 # Pas de warmup\n",
    "load_best_model_at_end = True # Charger le meilleur modèle parmi ceux générés pendant les epochs\n",
    "\n",
    "# Nombre de labels dans le dataset (ici on est sur de la classification binaire donc 2)\n",
    "num_labels = 2"
   ],
   "id": "d7d88b1cdff9f27a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Détection du matériel à disposition pour l'entrainement du modèle\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "use_cpu = device.type == \"cpu\"\n",
    "\n",
    "if str(device) == \"cuda\":\n",
    "    device_name = torch.cuda.get_device_name()\n",
    "else:\n",
    "    device_name = \"CPU nul...\"\n",
    "\n",
    "print(\"Device:\", device_name)"
   ],
   "id": "f8878c6e5a0b925e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import des données",
   "id": "3ab218439a11a07f"
  },
  {
   "cell_type": "code",
   "id": "fbdb436e8c923848",
   "metadata": {},
   "source": [
    "# Charger les données\n",
    "train_df = pd.read_csv(\"../preprocessing_with_BLIP/train_blip.csv\")\n",
    "test_df = pd.read_csv(\"../preprocessing_with_BLIP/test_blip.csv\")\n",
    "\n",
    "# Nettoyage des données\n",
    "train_df.dropna(inplace=True)\n",
    "test_df.dropna(inplace=True)\n",
    "\n",
    "# Compter le nombre d'exemples dans chaque classe dans le dataset d'entraînement\n",
    "train_label_counts = train_df[\"label\"].value_counts()\n",
    "print(\"Répartition des labels dans le dataset d'entraînement :\\n\", train_label_counts)\n",
    "print()\n",
    "\n",
    "# Compter le nombre d'exemples dans chaque classe dans le dataset de test\n",
    "test_label_counts = test_df[\"label\"].value_counts()\n",
    "print(\"Répartition des labels dans le dataset de test :\\n\", test_label_counts)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5e4df0ccbc2b6177",
   "metadata": {},
   "source": [
    "# Tokenizer de DistilBERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6f2950f5954366f9",
   "metadata": {},
   "source": [
    "class MemeDataset(Dataset):\n",
    "    def __init__(self, texts, descs, labels):\n",
    "        self.texts = texts\n",
    "        self.descs = descs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        desc = self.descs[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = tokenizer(desc, text, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f64a2ffcb392be30",
   "metadata": {},
   "source": [
    "## Préparation et création du modèle"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import DistilBertConfig\n",
    "\n",
    "# Séparer 10% des données d'entraînement pour la validation\n",
    "train_texts, val_texts, train_descs, val_descs, train_labels, val_labels = train_test_split(\n",
    "    train_df[\"text\"], train_df[\"img_desc\"], train_df[\"label\"], test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Créer les datasets pour Trainer\n",
    "train_dataset = MemeDataset(train_texts.tolist(), train_descs.tolist(), train_labels.tolist())\n",
    "val_dataset = MemeDataset(val_texts.tolist(), val_descs.tolist(), val_labels.tolist())\n",
    "test_dataset = MemeDataset(test_df[\"text\"].tolist(), test_df[\"img_desc\"].tolist(), test_df[\"label\"].tolist())\n",
    "\n",
    "# Charger la configuration du dropout\n",
    "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "config.dropout = dropout_global  # Appliquer du dropout globalement\n",
    "config.attention_dropout = dropout_attention  # Appliquer du dropout dans les couches d'attention\n",
    "config.num_labels=num_labels\n",
    "\n",
    "# Charger le modèle BERT (sur le device détecté)\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", config=config)\n",
    "model.to(device)"
   ],
   "id": "79d6d2d03eb39c43",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ],
   "id": "445c2f0ee79dc3da",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4370951f614bd027",
   "metadata": {},
   "source": [
    "# Configuration de l'entraînement optimisée pour CPU\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=num_epochs,  # Réduire à 1 époque pour éviter un entraînement trop long\n",
    "    per_device_train_batch_size=batch_size,  # Réduire pour éviter saturation mémoire\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    warmup_steps=warmup_steps,\n",
    "    weight_decay=weight_decay,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_dir=\"./logs\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    use_cpu=use_cpu,\n",
    "    load_best_model_at_end = True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=default_data_collator  # Ajout de cette ligne\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2edd5572f9962795",
   "metadata": {},
   "source": [
    "## Entrainement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "id": "fcd632931a879a10",
   "metadata": {},
   "source": [
    "# Entraînement du modèle (cela prendra du temps sur CPU)\n",
    "print(f\"Modèle chargé sur {device_name}\")\n",
    "print(\"Début de l'entrainement...\")\n",
    "trainer.train()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9cfb23c3b94e90e3",
   "metadata": {},
   "source": [
    "# Obtenir les prédictions sur le train set\n",
    "predictions = trainer.predict(train_dataset)\n",
    "logits = predictions.predictions\n",
    "y_train = np.argmax(logits, axis=-1)\n",
    "\n",
    "# Obtenir les prédictions sur le test set\n",
    "predictions = trainer.predict(test_dataset)\n",
    "logits = predictions.predictions\n",
    "y_pred = np.argmax(logits, axis=-1)\n",
    "\n",
    "# Calculer la distribution des prédictions pour le train et le test\n",
    "unique_train, counts_train = np.unique(y_train, return_counts=True)\n",
    "train_counts = dict(zip(unique_train, counts_train))\n",
    "\n",
    "unique_test, counts_test = np.unique(y_pred, return_counts=True)\n",
    "test_counts = dict(zip(unique_test, counts_test))\n",
    "\n",
    "\n",
    "\n",
    "# Charger la métrique d'accuracy\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# Obtenir les prédictions sur le train et dev set\n",
    "train_predictions = trainer.predict(train_dataset)\n",
    "train_logits = train_predictions.predictions\n",
    "train_labels = train_predictions.label_ids\n",
    "train_preds = np.argmax(train_logits, axis=-1)\n",
    "\n",
    "test_predictions = trainer.predict(test_dataset)\n",
    "test_logits = test_predictions.predictions\n",
    "test_labels = test_predictions.label_ids\n",
    "test_preds = np.argmax(test_logits, axis=-1)\n",
    "\n",
    "# Calculer l'accuracy\n",
    "train_acc = accuracy_metric.compute(predictions=train_preds, references=train_labels)['accuracy']\n",
    "test_acc = accuracy_metric.compute(predictions=test_preds, references=test_labels)['accuracy']\n",
    "\n",
    "# Calculer le F1 score\n",
    "train_f1 = f1_score(train_labels, train_preds, average='binary')\n",
    "test_f1 = f1_score(test_labels, test_preds, average='binary')\n",
    "\n",
    "# Distribution des prédictions en pourcentage\n",
    "def distribution(y_pred):\n",
    "    unique, counts = np.unique(y_pred, return_counts=True)\n",
    "    total = len(y_pred)\n",
    "    distribution = {label: round((count / total) * 100, 2) for label, count in zip(unique, counts)}\n",
    "    return distribution\n",
    "\n",
    "train_dist = distribution(train_preds)\n",
    "dev_dist = distribution(test_preds)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Afficher la distribution avec les pourcentages\n",
    "print(\"=== Distribution des prédictions sur TRAIN ===\")\n",
    "total_train = sum(counts_train)\n",
    "for label, count in train_counts.items():\n",
    "    percentage = (count / total_train) * 100\n",
    "    print(f\"Classe {label}: {count} ({percentage:.2f}%)\")\n",
    "print(\"=================================\")\n",
    "\n",
    "print(\"=== Distribution des prédictions sur TEST ===\")\n",
    "total_test = sum(counts_test)\n",
    "for label, count in test_counts.items():\n",
    "    percentage = (count / total_test) * 100\n",
    "    print(f\"Classe {label}: {count} ({percentage:.2f}%)\")\n",
    "print(\"=================================\")\n",
    "\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"===== Train Accuracy =====\")\n",
    "acc = round(train_acc * 100, 2)\n",
    "print(f\"Accuracy: {acc}%\")\n",
    "\n",
    "acc = round(train_f1 * 100, 2)\n",
    "print(f\"F1 Score: {acc}%\")\n",
    "\n",
    "print(\"\\n===== Dev Accuracy =====\")\n",
    "acc = round(test_acc * 100, 2)\n",
    "print(f\"Accuracy: {acc}%\")\n",
    "\n",
    "acc = round(test_f1 * 100, 2)\n",
    "print(f\"F1 Score: {acc}%\")"
   ],
   "id": "ab3485479d8a07e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b3559ece052302ef",
   "metadata": {},
   "source": [
    "## Export des résultats et hyperparamètres associés"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Fonction pour formater les dictionnaires contenant des np.int64\n",
    "def format_dict(d):\n",
    "    return \" , \".join(f\"{int(k)} : {int(v)}\" for k, v in d.items())\n",
    "\n",
    "# Hyperparamètres et résultats\n",
    "params = {\n",
    "    \"max_length\": max_length,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"num_epochs\": num_epochs,\n",
    "    \"dropout_global\": dropout_global,\n",
    "    \"dropout_attention\": dropout_attention,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"warmup_steps\": warmup_steps,\n",
    "    \"load_best_model_at_end\": load_best_model_at_end,\n",
    "    \"num_labels\": num_labels,\n",
    "    \"train_acc\": round(train_acc, 4),\n",
    "    \"train_f1_score\": round(train_f1, 4),\n",
    "    \"test_acc\": round(test_acc, 4),\n",
    "    \"test_f1_score\": round(test_f1, 4),\n",
    "    \"train_pred_distribution\": format_dict(train_counts),\n",
    "    \"test_pred_distribution\": format_dict(test_counts)\n",
    "}\n",
    "\n",
    "# Nom du fichier CSV\n",
    "output_file = \"hyperparameters_results.csv\"\n",
    "\n",
    "# Vérifier si le fichier existe déjà\n",
    "if os.path.exists(output_file):\n",
    "    # Lire le fichier existant\n",
    "    with open(output_file, mode='r', newline='', encoding='utf-8-sig') as file:\n",
    "        reader = csv.reader(file)\n",
    "        rows = list(reader)\n",
    "\n",
    "    # Déterminer le numéro de la prochaine colonne (valeurs_X)\n",
    "    header = rows[0]\n",
    "    existing_value_columns = [col for col in header[1:] if col.startswith(\"valeurs_\")]\n",
    "    if existing_value_columns:\n",
    "        last_index = max(int(col.split(\"_\")[1]) for col in existing_value_columns)\n",
    "        new_col_name = f\"valeurs_{last_index + 1}\"\n",
    "    else:\n",
    "        new_col_name = \"valeurs_1\"\n",
    "\n",
    "    # Ajouter le nom de la nouvelle colonne dans l'en-tête\n",
    "    header.append(new_col_name)\n",
    "\n",
    "    # Mettre à jour les lignes avec les nouvelles valeurs\n",
    "    for i in range(1, len(rows)):  # On commence à 1 pour ignorer l'en-tête\n",
    "        key = rows[i][0]\n",
    "        if key in params:\n",
    "            rows[i].append(params[key])  # Ajouter la valeur correspondante\n",
    "        else:\n",
    "            rows[i].append(\"\")  # Ajouter une cellule vide pour les lignes sans correspondance\n",
    "\n",
    "    # Écrire les nouvelles données dans le fichier\n",
    "    with open(output_file, mode='w', newline='', encoding='utf-8-sig') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(rows)\n",
    "\n",
    "else:\n",
    "    # Fichier n'existe pas encore, le créer et écrire les données\n",
    "    with open(output_file, mode='w', newline='', encoding='utf-8-sig') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        # Écrire l'en-tête\n",
    "        writer.writerow([\"Hyperparamètres et Résultats\", \"valeurs_1\"])\n",
    "\n",
    "        # Écrire les valeurs\n",
    "        for key, value in params.items():\n",
    "            writer.writerow([key, value])\n",
    "\n",
    "print(f\"Données exportées avec succès dans {output_file}!\")\n"
   ],
   "id": "700a2273d77fdc47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "fa72050b07dbad8c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
