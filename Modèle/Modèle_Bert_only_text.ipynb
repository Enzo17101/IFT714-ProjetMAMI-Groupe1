{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T20:43:17.956097Z",
     "start_time": "2025-03-21T20:43:10.225223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import default_data_collator\n",
    "from transformers import BertConfig, BertForSequenceClassification\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ],
   "id": "2400ebf221fe671f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Developpement\\Cours\\IFT714-NLP\\Devoir2\\.venv\\ProjetMAMI\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Hyperparamètres",
   "id": "2b350f28aac359f0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T20:43:17.964071Z",
     "start_time": "2025-03-21T20:43:17.960062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tokeniser\n",
    "max_length = 256 # Longueur maximale des séquences tokenisées\n",
    "\n",
    "# Capacité du modèle\n",
    "batch_size = 16 # Taille des batchs\n",
    "num_epochs = 3 # Nombre d'époques\n",
    "\n",
    "# Dropout\n",
    "dropout_global = 0.3 # Dropout global\n",
    "dropout_attention = 0.4 # Dropout dans les couches d'attention\n",
    "\n",
    "# Autres paramètres du modèle\n",
    "#learning_rate = 2e-5 # Taux d'apprentissage\n",
    "weight_decay = 0.1 # Paramètre de régularisation L2\n",
    "warmup_steps = 0 # Pas de warmup\n",
    "load_best_model_at_end = True # Charger le meilleur modèle parmi ceux générés pendant les epochs\n",
    "\n",
    "# Nombre de labels dans le dataset (ici on est sur de la classification binaire donc 2)\n",
    "num_labels = 2"
   ],
   "id": "d7d88b1cdff9f27a",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T20:43:18.160016Z",
     "start_time": "2025-03-21T20:43:18.124586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Détection du matériel à disposition pour l'entrainement du modèle\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "use_cpu = device.type == \"cpu\"\n",
    "\n",
    "if str(device) == \"cuda\":\n",
    "    device_name = torch.cuda.get_device_name()\n",
    "else:\n",
    "    device_name = \"CPU nul...\"\n",
    "\n",
    "print(\"Device:\", device_name)"
   ],
   "id": "f8878c6e5a0b925e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import des données",
   "id": "3ab218439a11a07f"
  },
  {
   "cell_type": "code",
   "id": "fbdb436e8c923848",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T20:43:18.200720Z",
     "start_time": "2025-03-21T20:43:18.166997Z"
    }
   },
   "source": [
    "# Charger les données\n",
    "train_df = pd.read_csv(\"../preprocessing_with_BLIP/train_blip.csv\")\n",
    "test_df = pd.read_csv(\"../preprocessing_with_BLIP/test_blip.csv\")\n",
    "\n",
    "# Nettoyage des données\n",
    "train_df.dropna(inplace=True)\n",
    "test_df.dropna(inplace=True)\n",
    "\n",
    "# Compter le nombre d'exemples dans chaque classe dans le dataset d'entraînement\n",
    "train_label_counts = train_df[\"label\"].value_counts()\n",
    "print(\"Répartition des labels dans le dataset d'entraînement :\\n\", train_label_counts)\n",
    "print()\n",
    "\n",
    "# Compter le nombre d'exemples dans chaque classe dans le dataset de test\n",
    "test_label_counts = test_df[\"label\"].value_counts()\n",
    "print(\"Répartition des labels dans le dataset de test :\\n\", test_label_counts)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Répartition des labels dans le dataset d'entraînement :\n",
      " label\n",
      "0    5000\n",
      "1    5000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Répartition des labels dans le dataset de test :\n",
      " label\n",
      "0    500\n",
      "1    500\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "5e4df0ccbc2b6177",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T20:43:18.521956Z",
     "start_time": "2025-03-21T20:43:18.215187Z"
    }
   },
   "source": [
    "# Tokenizer de DistilBERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "6f2950f5954366f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T20:43:18.533570Z",
     "start_time": "2025-03-21T20:43:18.529980Z"
    }
   },
   "source": [
    "class MemeDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "f64a2ffcb392be30",
   "metadata": {},
   "source": [
    "## Préparation et création du modèle"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T20:43:19.675173Z",
     "start_time": "2025-03-21T20:43:18.544618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import DistilBertConfig\n",
    "\n",
    "# Séparer 10% des données d'entraînement pour la validation\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_df[\"text\"], train_df[\"label\"], test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Créer les datasets pour Trainer\n",
    "train_dataset = MemeDataset(train_texts.tolist(), train_labels.tolist())\n",
    "val_dataset = MemeDataset(val_texts.tolist(), val_labels.tolist())\n",
    "test_dataset = MemeDataset(test_df[\"text\"].tolist(), test_df[\"label\"].tolist())\n",
    "\n",
    "# Charger la configuration du dropout\n",
    "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "config.dropout = dropout_global  # Appliquer du dropout globalement\n",
    "config.attention_dropout = dropout_attention  # Appliquer du dropout dans les couches d'attention\n",
    "config.num_labels=num_labels\n",
    "\n",
    "# Charger le modèle BERT (sur le device détecté)\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", config=config)\n",
    "model.to(device)"
   ],
   "id": "79d6d2d03eb39c43",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T20:43:19.688539Z",
     "start_time": "2025-03-21T20:43:19.685146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ],
   "id": "445c2f0ee79dc3da",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "4370951f614bd027",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T20:43:19.839947Z",
     "start_time": "2025-03-21T20:43:19.718226Z"
    }
   },
   "source": [
    "# Configuration de l'entraînement optimisée pour CPU\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=num_epochs,  # Réduire à 1 époque pour éviter un entraînement trop long\n",
    "    per_device_train_batch_size=batch_size,  # Réduire pour éviter saturation mémoire\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    warmup_steps=warmup_steps,\n",
    "    weight_decay=weight_decay,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_dir=\"./logs\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    use_cpu=use_cpu,\n",
    "    load_best_model_at_end = True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=default_data_collator  # Ajout de cette ligne\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "2edd5572f9962795",
   "metadata": {},
   "source": [
    "## Entrainement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "id": "fcd632931a879a10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T20:55:20.073897Z",
     "start_time": "2025-03-21T20:43:19.844916Z"
    }
   },
   "source": [
    "# Entraînement du modèle (cela prendra du temps sur CPU)\n",
    "print(f\"Modèle chargé sur {device_name}\")\n",
    "print(\"Début de l'entrainement...\")\n",
    "trainer.train()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle chargé sur NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Début de l'entrainement...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1689' max='1689' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1689/1689 11:59, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.508000</td>\n",
       "      <td>0.418176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.333700</td>\n",
       "      <td>0.449725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.202500</td>\n",
       "      <td>0.672543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1689, training_loss=0.32807967308500663, metrics={'train_runtime': 720.0553, 'train_samples_per_second': 37.497, 'train_steps_per_second': 2.346, 'total_flos': 3551999247360000.0, 'train_loss': 0.32807967308500663, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "9cfb23c3b94e90e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T20:57:58.430142Z",
     "start_time": "2025-03-21T20:55:20.234388Z"
    }
   },
   "source": [
    "# Obtenir les prédictions sur le train set\n",
    "predictions = trainer.predict(train_dataset)\n",
    "logits = predictions.predictions\n",
    "y_train = np.argmax(logits, axis=-1)\n",
    "\n",
    "# Obtenir les prédictions sur le test set\n",
    "predictions = trainer.predict(test_dataset)\n",
    "logits = predictions.predictions\n",
    "y_pred = np.argmax(logits, axis=-1)\n",
    "\n",
    "# Calculer la distribution des prédictions pour le train et le test\n",
    "unique_train, counts_train = np.unique(y_train, return_counts=True)\n",
    "train_counts = dict(zip(unique_train, counts_train))\n",
    "\n",
    "unique_test, counts_test = np.unique(y_pred, return_counts=True)\n",
    "test_counts = dict(zip(unique_test, counts_test))\n",
    "\n",
    "\n",
    "\n",
    "# Charger la métrique d'accuracy\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# Obtenir les prédictions sur le train et dev set\n",
    "train_predictions = trainer.predict(train_dataset)\n",
    "train_logits = train_predictions.predictions\n",
    "train_labels = train_predictions.label_ids\n",
    "train_preds = np.argmax(train_logits, axis=-1)\n",
    "\n",
    "test_predictions = trainer.predict(test_dataset)\n",
    "test_logits = test_predictions.predictions\n",
    "test_labels = test_predictions.label_ids\n",
    "test_preds = np.argmax(test_logits, axis=-1)\n",
    "\n",
    "# Calculer l'accuracy\n",
    "train_acc = accuracy_metric.compute(predictions=train_preds, references=train_labels)['accuracy']\n",
    "test_acc = accuracy_metric.compute(predictions=test_preds, references=test_labels)['accuracy']\n",
    "\n",
    "# Calculer le F1 score\n",
    "train_f1 = f1_score(train_labels, train_preds, average='binary')\n",
    "test_f1 = f1_score(test_labels, test_preds, average='binary')\n",
    "\n",
    "# Distribution des prédictions en pourcentage\n",
    "def distribution(y_pred):\n",
    "    unique, counts = np.unique(y_pred, return_counts=True)\n",
    "    total = len(y_pred)\n",
    "    distribution = {label: round((count / total) * 100, 2) for label, count in zip(unique, counts)}\n",
    "    return distribution\n",
    "\n",
    "train_dist = distribution(train_preds)\n",
    "dev_dist = distribution(test_preds)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T20:57:58.486717Z",
     "start_time": "2025-03-21T20:57:58.479714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Afficher la distribution avec les pourcentages\n",
    "print(\"=== Distribution des prédictions sur TRAIN ===\")\n",
    "total_train = sum(counts_train)\n",
    "for label, count in train_counts.items():\n",
    "    percentage = (count / total_train) * 100\n",
    "    print(f\"Classe {label}: {count} ({percentage:.2f}%)\")\n",
    "print(\"=================================\")\n",
    "\n",
    "print(\"=== Distribution des prédictions sur TEST ===\")\n",
    "total_test = sum(counts_test)\n",
    "for label, count in test_counts.items():\n",
    "    percentage = (count / total_test) * 100\n",
    "    print(f\"Classe {label}: {count} ({percentage:.2f}%)\")\n",
    "print(\"=================================\")\n",
    "\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"===== Train Accuracy =====\")\n",
    "acc = round(train_acc * 100, 2)\n",
    "print(f\"Accuracy: {acc}%\")\n",
    "\n",
    "acc = round(train_f1 * 100, 2)\n",
    "print(f\"F1 Score: {acc}%\")\n",
    "\n",
    "print(\"\\n===== Dev Accuracy =====\")\n",
    "acc = round(test_acc * 100, 2)\n",
    "print(f\"Accuracy: {acc}%\")\n",
    "\n",
    "acc = round(test_f1 * 100, 2)\n",
    "print(f\"F1 Score: {acc}%\")"
   ],
   "id": "ab3485479d8a07e1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Distribution des prédictions sur TRAIN ===\n",
      "Classe 0: 4674 (51.93%)\n",
      "Classe 1: 4326 (48.07%)\n",
      "=================================\n",
      "=== Distribution des prédictions sur TEST ===\n",
      "Classe 0: 366 (36.60%)\n",
      "Classe 1: 634 (63.40%)\n",
      "=================================\n",
      "===== Train Accuracy =====\n",
      "Accuracy: 88.29%\n",
      "F1 Score: 88.04%\n",
      "\n",
      "===== Dev Accuracy =====\n",
      "Accuracy: 66.4%\n",
      "F1 Score: 70.37%\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "b3559ece052302ef",
   "metadata": {},
   "source": [
    "## Export des résultats et hyperparamètres associés"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T20:57:58.512839Z",
     "start_time": "2025-03-21T20:57:58.494714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Fonction pour formater les dictionnaires contenant des np.int64\n",
    "def format_dict(d):\n",
    "    return \" , \".join(f\"{int(k)} : {int(v)}\" for k, v in d.items())\n",
    "\n",
    "# Hyperparamètres et résultats\n",
    "params = {\n",
    "    \"max_length\": max_length,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"num_epochs\": num_epochs,\n",
    "    \"dropout_global\": dropout_global,\n",
    "    \"dropout_attention\": dropout_attention,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"warmup_steps\": warmup_steps,\n",
    "    \"load_best_model_at_end\": load_best_model_at_end,\n",
    "    \"num_labels\": num_labels,\n",
    "    \"train_acc\": round(train_acc, 4),\n",
    "    \"train_f1_score\": round(train_f1, 4),\n",
    "    \"test_acc\": round(test_acc, 4),\n",
    "    \"test_f1_score\": round(test_f1, 4),\n",
    "    \"train_pred_distribution\": format_dict(train_counts),\n",
    "    \"test_pred_distribution\": format_dict(test_counts)\n",
    "}\n",
    "\n",
    "# Nom du fichier CSV\n",
    "output_file = \"hyperparameters_results.csv\"\n",
    "\n",
    "# Vérifier si le fichier existe déjà\n",
    "if os.path.exists(output_file):\n",
    "    # Lire le fichier existant\n",
    "    with open(output_file, mode='r', newline='', encoding='utf-8-sig') as file:\n",
    "        reader = csv.reader(file)\n",
    "        rows = list(reader)\n",
    "\n",
    "    # Déterminer le numéro de la prochaine colonne (valeurs_X)\n",
    "    header = rows[0]\n",
    "    existing_value_columns = [col for col in header[1:] if col.startswith(\"valeurs_\")]\n",
    "    if existing_value_columns:\n",
    "        last_index = max(int(col.split(\"_\")[1]) for col in existing_value_columns)\n",
    "        new_col_name = f\"valeurs_{last_index + 1}\"\n",
    "    else:\n",
    "        new_col_name = \"valeurs_1\"\n",
    "\n",
    "    # Ajouter le nom de la nouvelle colonne dans l'en-tête\n",
    "    header.append(new_col_name)\n",
    "\n",
    "    # Mettre à jour les lignes avec les nouvelles valeurs\n",
    "    for i in range(1, len(rows)):  # On commence à 1 pour ignorer l'en-tête\n",
    "        key = rows[i][0]\n",
    "        if key in params:\n",
    "            rows[i].append(params[key])  # Ajouter la valeur correspondante\n",
    "        else:\n",
    "            rows[i].append(\"\")  # Ajouter une cellule vide pour les lignes sans correspondance\n",
    "\n",
    "    # Écrire les nouvelles données dans le fichier\n",
    "    with open(output_file, mode='w', newline='', encoding='utf-8-sig') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(rows)\n",
    "\n",
    "else:\n",
    "    # Fichier n'existe pas encore, le créer et écrire les données\n",
    "    with open(output_file, mode='w', newline='', encoding='utf-8-sig') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        # Écrire l'en-tête\n",
    "        writer.writerow([\"Hyperparamètres et Résultats\", \"valeurs_1\"])\n",
    "\n",
    "        # Écrire les valeurs\n",
    "        for key, value in params.items():\n",
    "            writer.writerow([key, value])\n",
    "\n",
    "print(f\"Données exportées avec succès dans {output_file}!\")\n"
   ],
   "id": "700a2273d77fdc47",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données exportées avec succès dans hyperparameters_results.csv!\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T20:57:58.545852Z",
     "start_time": "2025-03-21T20:57:58.541838Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "fa72050b07dbad8c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
