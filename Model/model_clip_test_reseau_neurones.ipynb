{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e69de2f5900d2fd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:35:34.582988Z",
     "start_time": "2025-03-27T16:35:18.970401Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # Importer tqdm pour la barre de progression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9b5a7645904d6c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:35:34.649039Z",
     "start_time": "2025-03-27T16:35:34.602437Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Répartition des labels dans le dataset d'entraînement :\n",
      " label\n",
      "0    5000\n",
      "1    5000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Charger les données\n",
    "train_df = pd.read_csv(\"../preprocessing_for_CLIP/train.csv\")\n",
    "test_texts_df = pd.read_csv(\"../preprocessing_for_CLIP/test_texts.csv\")\n",
    "test_labels_df = pd.read_csv(\"../preprocessing_for_CLIP/test_labels.csv\")\n",
    "train_folder_path = \"../dataset/TRAINING\"\n",
    "test_folder_path = \"../dataset/test\"\n",
    "\n",
    "\n",
    "# Compter le nombre d'exemples dans chaque classe\n",
    "print(\"Répartition des labels dans le dataset d'entraînement :\\n\", train_df[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95fd8d7cfaecaa0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:36:37.580490Z",
     "start_time": "2025-03-27T16:36:35.121092Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du modèle CLIP...\n",
      "Modèle CLIP chargé !\n"
     ]
    }
   ],
   "source": [
    "# Charger le modèle CLIP et le processor pour prétraiter les images et textes\n",
    "print(\"Chargement du modèle CLIP...\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "print(\"Modèle CLIP chargé !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d48c7f9d4d837f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:51:33.106511Z",
     "start_time": "2025-03-27T16:51:33.097189Z"
    }
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "def image_path_to_tensor(image_path, folder_path):\n",
    "    # Charger l'image avec PIL\n",
    "    full_image_path = os.path.join(folder_path, image_path)\n",
    "    image = Image.open(full_image_path).convert(\"RGB\")\n",
    "    image_np = np.array(image, dtype=np.float32) / 255.0 \n",
    "    # Vérifier si les valeurs sont bien dans la plage [0, 1]\n",
    "    if np.any(image_np < 0) or np.any(image_np > 1):\n",
    "        print(f\"Attention: image avec valeurs hors de la plage [0, 1] dans {image_path}\")\n",
    "    # Utiliser le préprocesseur CLIP pour transformer l'image\n",
    "    inputs = processor(images=image, return_tensors=\"pt\", padding=True, do_rescale=False)  # Désactiver la mise à l'échelle\n",
    "\n",
    "    return inputs['pixel_values'][0]  # Retourne le tensor de l'image\n",
    "\n",
    "def get_clip_embeddings(texts, images):\n",
    "    # Prétraiter les textes séparément\n",
    "    text_inputs = processor(text=texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=77)\n",
    "    \n",
    "    # Normaliser les images dans la plage [0, 1]\n",
    "    # La normalisation est effectuée à la place avant de les transmettre au préprocesseur CLIP\n",
    "    images_normalized = []\n",
    "    for image in images:\n",
    "        # Assurez-vous que les valeurs sont dans la plage [0, 1] avant d'être envoyées au modèle\n",
    "        if isinstance(image, np.ndarray):\n",
    "            image = np.clip(image / 255.0, 0.0, 1.0)  # Normaliser et contraindre entre [0, 1]\n",
    "        else:\n",
    "            image = np.clip(np.array(image) / 255.0, 0.0, 1.0)\n",
    "        images_normalized.append(image)\n",
    "\n",
    "    # Prétraiter les images séparément\n",
    "    image_inputs = processor(images=images_normalized, return_tensors=\"pt\", padding=True, do_rescale=False)  # Désactiver la mise à l'échelle\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Obtenir les embeddings des textes et des images\n",
    "        text_embeds = model.get_text_features(**text_inputs)\n",
    "        image_embeds = model.get_image_features(**image_inputs)\n",
    "\n",
    "    return text_embeds, image_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29488d171c478581",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:37:02.815372Z",
     "start_time": "2025-03-27T16:37:02.809737Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset pour charger les textes et les images\n",
    "class MemeDataset(Dataset):\n",
    "    def __init__(self, texts, images, labels):\n",
    "        self.texts = texts\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        return text, image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be10913d4d451487",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:40:58.052585Z",
     "start_time": "2025-03-27T16:37:03.411341Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des images de train\n",
      "Chargement des images de test\n"
     ]
    }
   ],
   "source": [
    "# Charger les images (supposons qu'elles sont en format chemin d'image ou déjà converties en tensors)\n",
    "print(\"Chargement des images de train\")\n",
    "train_images = [image_path_to_tensor(image_path, train_folder_path) for image_path in train_df[\"file_name\"]]\n",
    "print(\"Chargement des images de test\")\n",
    "test_images = [image_path_to_tensor(image_path, test_folder_path) for image_path in test_texts_df[\"file_name\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63ae358382d55bf7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:41:22.795523Z",
     "start_time": "2025-03-27T16:41:22.784651Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Préparation des datasets...\n"
     ]
    }
   ],
   "source": [
    "# Préparer les datasets\n",
    "print(\"Préparation des datasets...\")\n",
    "train_texts = train_df[\"text\"].tolist()\n",
    "train_labels = train_df[\"label\"].tolist()\n",
    "\n",
    "test_texts = test_texts_df[\"text\"].tolist()\n",
    "test_labels = test_labels_df[\"label\"].tolist()\n",
    "\n",
    "train_dataset = MemeDataset(train_texts, train_images, train_labels)\n",
    "test_dataset = MemeDataset(test_texts, test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62b9243f31978fb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:41:30.016444Z",
     "start_time": "2025-03-27T16:41:30.009966Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extraire les embeddings et entraîner le modèle\n",
    "def extract_embeddings(loader):\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    # Ajouter tqdm pour la barre de progression\n",
    "    for texts, images, label in tqdm(loader, desc=\"Extraction des embeddings\", leave=False):\n",
    "        text_embeds, image_embeds = get_clip_embeddings(texts, images)\n",
    "        combined_embeds = torch.cat((text_embeds, image_embeds), dim=-1).numpy()  # Combine embeddings\n",
    "        embeddings.append(combined_embeds)\n",
    "        labels.append(label.numpy())\n",
    "    return np.vstack(embeddings), np.hstack(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1675fdfda4c33635",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T17:13:42.771270Z",
     "start_time": "2025-03-27T16:52:43.406622Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des embeddings sauvegardés...\n",
      "Chargement des embeddings terminé !\n"
     ]
    }
   ],
   "source": [
    "# Chemins des fichiers pour sauvegarder les embeddings\n",
    "train_embeddings_file = \"train_embeddings.npz\"\n",
    "test_embeddings_file = \"test_embeddings.npz\"\n",
    "\n",
    "# Vérifier si les fichiers existent déjà\n",
    "if os.path.exists(train_embeddings_file) and os.path.exists(test_embeddings_file):\n",
    "    print(\"Chargement des embeddings sauvegardés...\")\n",
    "    train_data = np.load(train_embeddings_file)\n",
    "    test_data = np.load(test_embeddings_file)\n",
    "    X_train, y_train = train_data[\"X\"], train_data[\"y\"]\n",
    "    X_test, y_test = test_data[\"X\"], test_data[\"y\"]\n",
    "    print(\"Chargement des embeddings terminé !\")\n",
    "else:\n",
    "    print(\"Extraction des embeddings d'entraînement...\")\n",
    "    X_train, y_train = extract_embeddings(train_loader)\n",
    "    print(\"Extraction des embeddings de test...\")\n",
    "    X_test, y_test = extract_embeddings(test_loader)\n",
    "    \n",
    "    print(\"Extraction des embeddings terminée !\")\n",
    "    \n",
    "    # Sauvegarde des embeddings\n",
    "    np.savez_compressed(train_embeddings_file, X=X_train, y=y_train)\n",
    "    np.savez_compressed(test_embeddings_file, X=X_test, y=y_test)\n",
    "    print(\"Embeddings sauvegardés !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7278c5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Définition du modèle de réseau de neurones\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, output_dim=1, dropout=0.3):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Initialisation du modèle\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "input_dim = 512 * 2  # Taille des embeddings CLIP (texte + image)\n",
    "model = NeuralNet(input_dim).to(device)\n",
    "\n",
    "# Définition de la fonction de perte et de l'optimiseur\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8af48344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entraînement du réseau de neurones...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progression:   0%|          | 0/1000 [00:00<?, ?epoch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progression: 100%|██████████| 1000/1000 [00:47<00:00, 21.18epoch/s, loss=0.00512]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entraînement du réseau de neurones terminé !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Convertir en tenseurs PyTorch\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "# Entraînement du réseau de neurones\n",
    "num_epochs = 1000\n",
    "batch_size = 16\n",
    "\n",
    "print(\"Entraînement du réseau de neurones...\")\n",
    "progress_bar = tqdm(range(num_epochs), desc=\"Progression\", unit=\"epoch\")\n",
    "\n",
    "for epoch in progress_bar:\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Prédiction et calcul de la perte\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "\n",
    "    # Rétropropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Mettre à jour tqdm avec la perte actuelle\n",
    "    progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "print(\"Entraînement du réseau de neurones terminé !\")\n",
    "\n",
    "# Évaluation sur les données de test\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_train = model(X_train_tensor).cpu().numpy().round()\n",
    "    y_pred_test = model(X_test_tensor).cpu().numpy().round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6368e3d00c6b781f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T17:42:19.761959Z",
     "start_time": "2025-03-27T17:42:16.274636Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 0.689\n",
      "Train F1 Score: 1.0\n",
      "Test F1 Score: 0.7371090448013525\n"
     ]
    }
   ],
   "source": [
    "# Calculer les métriques\n",
    "train_acc = accuracy_score(y_train, y_pred_train)\n",
    "test_acc = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "train_f1 = f1_score(y_train, y_pred_train, average='binary')\n",
    "test_f1 = f1_score(y_test, y_pred_test, average='binary')\n",
    "\n",
    "print(f\"Train Accuracy: {train_acc}\")\n",
    "print(f\"Test Accuracy: {test_acc}\")\n",
    "print(f\"Train F1 Score: {train_f1}\")\n",
    "print(f\"Test F1 Score: {test_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1211e61ce3abed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données exportées avec succès dans hyperparameters_results.csv !\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarde des résultats\n",
    "import csv\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "# Fonction pour formater les dictionnaires contenant des np.int64\n",
    "def format_dict(d):\n",
    "    return \" , \".join(f\"{int(k)} : {int(v)}\" for k, v in d.items())\n",
    "\n",
    "# Distribution des prédictions\n",
    "train_counts = Counter(y_pred_train.flatten())\n",
    "test_counts = Counter(y_pred_test.flatten())\n",
    "\n",
    "# Hyperparamètres et résultats\n",
    "params = {\n",
    "    \"batch_size\": batch_size,\n",
    "    \"hidden_dim\": 128,\n",
    "    \"dropout\": 0.3,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"lr\": 0.001,\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"num_epochs\": num_epochs,\n",
    "    \"train_acc\": round(train_acc, 4),\n",
    "    \"train_f1_score\": round(train_f1, 4),\n",
    "    \"test_acc\": round(test_acc, 4),\n",
    "    \"test_f1_score\": round(test_f1, 4),\n",
    "    \"train_pred_distribution\": format_dict(train_counts),\n",
    "    \"test_pred_distribution\": format_dict(test_counts)\n",
    "}\n",
    "\n",
    "# Sauvegarde dans un fichier CSV\n",
    "output_file = \"hyperparameters_results.csv\"\n",
    "\n",
    "if os.path.exists(output_file):\n",
    "    with open(output_file, mode='r', newline='', encoding='utf-8-sig') as file:\n",
    "        reader = csv.reader(file)\n",
    "        rows = list(reader)\n",
    "\n",
    "    header = rows[0]\n",
    "    existing_value_columns = [col for col in header[1:] if col.startswith(\"valeurs_\")]\n",
    "    new_col_name = f\"valeurs_{len(existing_value_columns) + 1}\" if existing_value_columns else \"valeurs_1\"\n",
    "    header.append(new_col_name)\n",
    "\n",
    "    for i in range(1, len(rows)):\n",
    "        key = rows[i][0]\n",
    "        rows[i].append(params.get(key, \"\"))\n",
    "\n",
    "    with open(output_file, mode='w', newline='', encoding='utf-8-sig') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(rows)\n",
    "else:\n",
    "    with open(output_file, mode='w', newline='', encoding='utf-8-sig') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Hyperparamètres et Résultats\", \"valeurs_1\"])\n",
    "        for key, value in params.items():\n",
    "            writer.writerow([key, value])\n",
    "\n",
    "print(f\"Données exportées avec succès dans {output_file} !\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
